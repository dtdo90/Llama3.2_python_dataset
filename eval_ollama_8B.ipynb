{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHRO-aE0Qpin",
        "outputId": "7c94a3f7-286f-4ae6-a737-6dd7cd8db65f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fEopjmuQriu",
        "outputId": "dc3fa01e-239f-4a9e-bd7b-0fe0eee2c77f"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/fine_tune_llms/llama3.2_python/\n",
        "\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "\n",
        "with open(\"config.json\", \"r\") as config_file:\n",
        "    config = json.load(config_file)\n",
        "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
        "login(token=access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUlgNxDBSmhU",
        "outputId": "86cfc405-1e70-44b2-e214-dc9276a5bdda"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asUtRrWgQ9L4"
      },
      "source": [
        "## 1. Download test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6fHG0Tf5Do",
        "outputId": "7defde68-aa79-4979-965d-0b5d323d1b3d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# download and saved test data\n",
        "def download_data():\n",
        "    # download data\n",
        "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
        "    ds_test=[]\n",
        "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
        "    num_samples=16800\n",
        "    counter=0\n",
        "    for sample in iter(ds):\n",
        "        if counter<num_samples:\n",
        "            counter+=1\n",
        "            continue\n",
        "        ds_test.append(sample)\n",
        "\n",
        "    data_file=\"data-test.json\"\n",
        "    with open(data_file, \"w\") as train_f:\n",
        "        json.dump(ds_test, train_f, indent=4)\n",
        "    print(f\"Training data saved to {data_file}\")\n",
        "    \n",
        "download_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UOtckuH5hYDQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def format_sample(sample):\n",
        "    \"\"\" Helper function to format a single input sample\"\"\"\n",
        "    instruction=sample['instruction']\n",
        "    input_text=sample['input']\n",
        "\n",
        "    if input_text is None or input_text==\"\":\n",
        "        formatted_prompt=(\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        )\n",
        "    else:\n",
        "        formatted_prompt=(\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
        "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        )\n",
        "    formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
        "    return formatted_prompt                    # stream text into the dataloader, one by one\n",
        "\n",
        "\n",
        "def generate_ft(model, sample, tokenizer, max_new_tokens, context_size=256, temperature=0.0, top_k=1, eos_id=[128001, 128009]):\n",
        "    \"\"\"\n",
        "    Generate text using a language model with proper dtype handling\n",
        "    \"\"\"\n",
        "    # Get model's expected dtype and device\n",
        "    model_dtype = next(model.parameters()).dtype\n",
        "    model_device = next(model.parameters()).device\n",
        "\n",
        "    formatted_prompt = format_sample(sample)\n",
        "\n",
        "    # Encode and prepare input\n",
        "    idx = tokenizer.encode(formatted_prompt)\n",
        "    idx = torch.tensor(idx, dtype=torch.long, device=model_device).unsqueeze(0)\n",
        "    num_tokens = idx.shape[1]\n",
        "\n",
        "    # Generation loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]   # conditioning context\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=idx_cond,use_cache=False)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Focus on last time step\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k is not None and top_k > 0:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, [-1]]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(float('-inf'), device=model_device, dtype=model_dtype),\n",
        "                logits\n",
        "            )\n",
        "\n",
        "        # Apply temperature and sample\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Check for EOS\n",
        "        if idx_next.item() in eos_id:\n",
        "            break\n",
        "\n",
        "        # Append new token\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    # Decode generated text\n",
        "    generated_ids = idx.squeeze(0)[num_tokens:]\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25B0jb4i2Y-"
      },
      "source": [
        "## 2. Load fine-tuned model and generate response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwg3Q_9zV4Ac",
        "outputId": "5d401a51-9eea-401c-a5f3-89d9761dff5e"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def load_fine_tune_model(base_model_id, saved_weights):\n",
        "    # Load tokenizer and base model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "    base_model.to(device)\n",
        "\n",
        "    # Create LoRA config - make sure these parameters match your training configuration\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        r=8,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=['q_proj','k_proj','v_proj'],\n",
        "    )\n",
        "\n",
        "    # Initialize PeftModel\n",
        "    lora_model = PeftModel(base_model, peft_config)\n",
        "\n",
        "    # Load the saved weights\n",
        "    state_dict = torch.load(saved_weights,map_location=device)\n",
        "\n",
        "    # Create new state dict with correct prefixes and structure\n",
        "    new_state_dict = {}\n",
        "    for key, value in state_dict.items():\n",
        "        # key starts with \"model\"-> add \"base_\" to the new key for base_model\n",
        "        new_key = f\"base_{key}\"\n",
        "        new_state_dict[new_key] = value\n",
        "\n",
        "    # Load the weights with strict=False to allow partial loading\n",
        "    lora_model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    # Set to evaluation mode\n",
        "    lora_model = lora_model.eval()\n",
        "\n",
        "    return lora_model, tokenizer\n",
        "\n",
        "# Original model and saved weight\n",
        "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "lora_weights = \"LLAMA32_ft_python_code.pth\"\n",
        "# Load model\n",
        "print(\"Loading fine-tuned model ...\")\n",
        "model_ft, tokenizer = load_fine_tune_model(base_model_id, lora_weights)\n",
        "total_params=sum(p.numel() for p in model_ft.parameters())\n",
        "trainable_params=sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "O3rPDbVNROow",
        "outputId": "e2d04d9c-6ddb-4db9-e180-7f1e2455e42f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# load test data\n",
        "test_data_path=\"data-test.json\"\n",
        "with open(test_data_path,\"r\") as f:\n",
        "    test_data=json.load(f)\n",
        "\n",
        "for i,sample in tqdm(enumerate(test_data),total=len(test_data)):\n",
        "    generated_text=generate_ft(model_ft, sample, tokenizer, max_new_tokens=100)\n",
        "    test_data[i][\"model response\"]=generated_text\n",
        "\n",
        "# write into a file\n",
        "test_data_path=\"test-data-with-response.json\"\n",
        "\n",
        "with open(test_data_path,\"w\") as file:\n",
        "    json.dump(test_data,file, indent=4)\n",
        "print(f\"Response saved as {test_data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_LP6gzDWtF-"
      },
      "source": [
        "## 3. Evaluate response with LLAMA3 8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MymNEGElQgG6",
        "outputId": "4ad82240-0572-49d6-bb23-198a62749ee8"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "\n",
        "def query_model(prompt, model=\"llama3.2\", url=\"http://localhost:11434/api/chat\"):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data\n",
        "\n",
        "\n",
        "result = query_model(\"What do Llamas eat?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVawkBVPQgHD",
        "outputId": "f3263da8-299c-48cd-b18f-a2686f5a6ef6"
      },
      "outputs": [],
      "source": [
        "# load test entries\n",
        "json_file = \"test-data-with-response.json\"\n",
        "\n",
        "with open(json_file, \"r\") as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "print(\"Number of entries:\", len(json_data))\n",
        "print(f\"First entry:\")\n",
        "json_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg7_EHcQQgHF",
        "outputId": "feeac867-3265-448a-cd38-8da86598fd84"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. Write a response that \"\n",
        "        f\"appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "    instruction_text + input_text\n",
        "\n",
        "    return instruction_text + input_text\n",
        "\n",
        "for entry in json_data[:3]:\n",
        "    prompt = (f\"Given the input `{format_input(entry)}` \"\n",
        "              f\"and correct output `{entry['output']}`, \"\n",
        "              f\"score the model response `{entry['model response']}`\"\n",
        "              f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "              )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", query_model(prompt))\n",
        "    print(\"\\n-------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HW_Kx4FQgHH",
        "outputId": "89b259d2-655c-4542-8bef-1710f4346d05"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_model_scores(json_data, json_key, sample_size=None):\n",
        "    \"\"\"\n",
        "    Generates model scores based on a subset or the full dataset.\n",
        "\n",
        "    Args:\n",
        "        json_data (list): The input data as a list of dictionaries.\n",
        "        json_key (str): The key in the dictionary to evaluate.\n",
        "        sample_size (int, optional): Number of random samples to evaluate. \n",
        "                                     If None, all entries in `json_data` are evaluated.\n",
        "\n",
        "    Returns: A list of scores for the evaluated entries.\n",
        "    \"\"\"\n",
        "    # If sample_size is None, evaluate the full dataset\n",
        "    if sample_size is None:\n",
        "        sampled_data = json_data\n",
        "    else:\n",
        "        sampled_data = random.sample(json_data, min(sample_size, len(json_data)))\n",
        "\n",
        "    scores = []\n",
        "    for entry in tqdm(sampled_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = generate_model_scores(json_data, \"model response\", sample_size=100)\n",
        "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
        "\n",
        "\n",
        "# # Optionally save the scores\n",
        "# save_path = Path(\"scores\") / f\"llama3.2-1b-model-reponse.json\"\n",
        "\n",
        "# # Create the parent directory if it doesn't exist\n",
        "# save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# with open(save_path, \"w\") as file:\n",
        "#     json.dump(scores, file)\n",
        "# print(f\"Responses saved at {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GNN_M1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
