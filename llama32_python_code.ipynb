{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-10T06:05:23.054047Z",
     "iopub.status.busy": "2025-01-10T06:05:23.053607Z",
     "iopub.status.idle": "2025-01-10T06:05:35.579473Z",
     "shell.execute_reply": "2025-01-10T06:05:35.578946Z",
     "shell.execute_reply.started": "2025-01-10T06:05:23.054026Z"
    },
    "id": "7KV84P94LouX",
    "outputId": "27440134-7f06-4cef-b8b8-03df881989c0"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-10T06:18:32.057359Z",
     "iopub.status.busy": "2025-01-10T06:18:32.056686Z",
     "iopub.status.idle": "2025-01-10T06:18:32.313751Z",
     "shell.execute_reply": "2025-01-10T06:18:32.313245Z",
     "shell.execute_reply.started": "2025-01-10T06:18:32.057338Z"
    },
    "id": "W2yGmrJ9LZgl",
    "outputId": "c577413f-968b-4af1-c918-2793b1ad9dc5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUDTL3n8MLLv"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T07:16:03.345595Z",
     "iopub.status.busy": "2025-01-10T07:16:03.344939Z",
     "iopub.status.idle": "2025-01-10T07:16:03.377782Z",
     "shell.execute_reply": "2025-01-10T07:16:03.377234Z",
     "shell.execute_reply.started": "2025-01-10T07:16:03.345575Z"
    },
    "id": "ooBAfXQxL00m"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "def format_sample(sample):\n",
    "    \"\"\" Helper function to format a single input sample\"\"\"\n",
    "    instruction=sample['instruction']\n",
    "    input_text=sample['input']\n",
    "    output_text=sample['output']\n",
    "\n",
    "    if input_text is None or input_text==\"\":\n",
    "        formatted_prompt=(\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{output_text}<|eot_id|><|start_header_id|>\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt=(\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
    "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{output_text}<|eot_id|><|end_of_text|>\"\n",
    "        )\n",
    "    formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
    "    return formatted_prompt                    # stream text into the dataloader, one by one\n",
    "\n",
    "\n",
    "\n",
    "def gen_train_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return:\n",
    "            A generator on train data \"train_gen\"\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
    "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
    "    num_samples=16800\n",
    "    counter=0\n",
    "    for sample in iter(ds):\n",
    "        if counter>=num_samples:\n",
    "            break\n",
    "        formatted_prompt=format_sample(sample)\n",
    "        yield {'text': formatted_prompt}\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "def gen_val_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return:\n",
    "            A generator on val data \"val_gen\"\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
    "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
    "    num_samples=16800\n",
    "    counter=0\n",
    "    for sample in iter(ds):\n",
    "        if counter<num_samples:\n",
    "            counter+=1\n",
    "            continue\n",
    "\n",
    "        formatted_prompt=format_sample(sample)\n",
    "        yield {'text': formatted_prompt}\n",
    "        counter+=1\n",
    "\n",
    "dataset_train = Dataset.from_generator(gen_train_input)\n",
    "dataset_val=Dataset.from_generator(gen_val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-10T07:16:05.092042Z",
     "iopub.status.busy": "2025-01-10T07:16:05.091415Z",
     "iopub.status.idle": "2025-01-10T07:16:05.095650Z",
     "shell.execute_reply": "2025-01-10T07:16:05.094984Z",
     "shell.execute_reply.started": "2025-01-10T07:16:05.092024Z"
    },
    "id": "GwPH7uvML02s",
    "outputId": "2dd352d8-a949-41f8-bb76-68196b253c59"
   },
   "outputs": [],
   "source": [
    "print(f\"Train dataset size: {len(dataset_train)}\")\n",
    "print(f\"Validation dataset size: {len(dataset_val)}\")\n",
    "\n",
    "print(f\"Sample train:\\n{dataset_train[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyqrYyOvSYFO"
   },
   "source": [
    "## Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T07:16:10.640508Z",
     "iopub.status.busy": "2025-01-10T07:16:10.639895Z",
     "iopub.status.idle": "2025-01-10T07:16:13.163344Z",
     "shell.execute_reply": "2025-01-10T07:16:13.162733Z",
     "shell.execute_reply.started": "2025-01-10T07:16:10.640489Z"
    },
    "id": "6ZIveBRhL04g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments,BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_name=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "def create_and_prepare_model():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        token=access_token\n",
    "    )\n",
    "\n",
    "    peft_config=LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
    "    )\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "    tokenizer.add_special_tokens\n",
    "    tokenizer.pad_token=\"<|end_of_text|>\" # this token is already available in tokenizer list\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model,peft_config,tokenizer\n",
    "\n",
    "model,peft_config,tokenizer=create_and_prepare_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-q42NQzSbK_"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "args=SFTConfig(\n",
    "    output_dir=\"./llama32-python\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True, # to save memmory\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    # Add evaluation strategy to compute validation loss during training\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at the end of each epoch\n",
    "    eval_steps=50,  # Evaluate every 50 steps    \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=False, # enable true for faster speed (supported in higher-end gpu)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03, # follow QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T07:16:27.951141Z",
     "iopub.status.busy": "2025-01-10T07:16:27.950691Z",
     "iopub.status.idle": "2025-01-10T09:40:20.949258Z",
     "shell.execute_reply": "2025-01-10T09:40:20.948752Z",
     "shell.execute_reply.started": "2025-01-10T07:16:27.951123Z"
    },
    "id": "Vma9uK6yaXLS"
   },
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finetuned model dict\n",
    "model_file_name=\"LLAMA32_ft_python_code.pth\"\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "print(f\"Model saved as {model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load finetune model and generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_fine_tune_model(base_model_id, saved_weights):\n",
    "    # Load tokenizer and base model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "    base_model.to(device)\n",
    "    \n",
    "    # Create LoRA config - make sure these parameters match your training configuration\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj','k_proj','v_proj'],\n",
    "    )\n",
    "    \n",
    "    # Initialize PeftModel\n",
    "    lora_model = PeftModel(base_model, peft_config)\n",
    "    \n",
    "    # Load the saved weights\n",
    "    state_dict = torch.load(saved_weights,map_location=device)\n",
    "        \n",
    "    # Create new state dict with correct prefixes and structure\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # key start with \"model\"-> add \"base_\" to the new key for base_model\n",
    "        new_key = f\"base_{key}\"        \n",
    "        new_state_dict[new_key] = value\n",
    "    \n",
    "    # Load the weights with strict=False to allow partial loading\n",
    "    lora_model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    lora_model = lora_model.eval()\n",
    "    \n",
    "    return lora_model, tokenizer\n",
    "\n",
    "# Original model and saved weight\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lora_weights = \"LLAMA32_ft_python_code.pth\"\n",
    "# Load model\n",
    "print(\"Loading fine-tuned model ...\")\n",
    "model_ft, tokenizer = load_fine_tune_model(base_model_id, lora_weights)\n",
    "total_params=sum(p.numel() for p in model_ft.parameters())\n",
    "trainable_params=sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T10:22:50.309140Z",
     "iopub.status.busy": "2025-01-10T10:22:50.308913Z",
     "iopub.status.idle": "2025-01-10T10:22:56.448695Z",
     "shell.execute_reply": "2025-01-10T10:22:56.448328Z",
     "shell.execute_reply.started": "2025-01-10T10:22:50.309125Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_ft(model, prompt, tokenizer, max_new_tokens, context_size=256, temperature=0.0, top_k=1, eos_id=[128001, 128009]):\n",
    "    \"\"\"\n",
    "    Generate text using a language model with proper dtype handling \n",
    "    \"\"\"\n",
    "    # Get model's expected dtype and device\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    model_device = next(model.parameters()).device\n",
    "    \n",
    "    formatted_prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{prompt}\"\n",
    "        f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Encode and prepare input\n",
    "    idx = tokenizer.encode(formatted_prompt)\n",
    "    idx = torch.tensor(idx, dtype=torch.long, device=model_device).unsqueeze(0)\n",
    "    num_tokens = idx.shape[1]\n",
    "    \n",
    "    # Generation loop\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   # conditioning context        \n",
    "        with torch.no_grad():\n",
    "            # Forward pass \n",
    "            outputs = model(input_ids=idx_cond,use_cache=False)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Focus on last time step\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, [-1]]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf'), device=model_device, dtype=model_dtype),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # Apply temperature and sample\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Check for EOS\n",
    "        if idx_next.item() in eos_id:\n",
    "            break\n",
    "            \n",
    "        # Append new token\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_ids = idx.squeeze(0)[num_tokens:]\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "prompt = \"Program a Flask API that will convert an image to grayscale.\"\n",
    "print(generate_ft(model_ft, prompt, tokenizer, max_new_tokens=256))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GNN_M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
